{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":860,"status":"ok","timestamp":1739296844382,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"},"user_tz":-60},"id":"IIMqcJCihkJ6"},"outputs":[],"source":["from transformers import BertTokenizer, BertModel, AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n","model = BertModel.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4465,"status":"ok","timestamp":1739297127682,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"},"user_tz":-60},"id":"Ct6g42yeX3Z0","outputId":"c9bae82f-bd44-4608-9f3e-0d8dc32c6e5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nlu-benchmark'...\n","remote: Enumerating objects: 400, done.\u001b[K\n","remote: Counting objects: 100% (11/11), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 400 (delta 2), reused 11 (delta 2), pack-reused 389 (from 1)\u001b[K\n","Receiving objects: 100% (400/400), 1.19 MiB | 1.33 MiB/s, done.\n","Resolving deltas: 100% (248/248), done.\n"]}],"source":["!pip install -q datasets\n","!git clone https://github.com/sonos/nlu-benchmark.git\n","# !pip install -q torch"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uzlH2PTYfif3","executionInfo":{"status":"ok","timestamp":1739297129381,"user_tz":-60,"elapsed":181,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["import json\n","from datasets import Dataset\n","\n","final_data = []\n","intent_list = []\n","intentpath_list = [\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/train_PlayMusic_full.json\",\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/train_AddToPlaylist_full.json\",\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/train_BookRestaurant_full.json\",\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/GetWeather/train_GetWeather_full.json\",\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/RateBook/train_RateBook_full.json\",\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/train_SearchCreativeWork_full.json\",\n","    \"/content/nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/train_SearchScreeningEvent_full.json\"\n","]\n","\n","for path in intentpath_list:\n","    with open(path, \"r\", encoding=\"latin-1\") as f:\n","        data = json.load(f)\n","\n","    intent_name = (list(data.keys())[0])\n","    data = data[intent_name]\n","    for i in data:\n","        i['intent'] = intent_name\n","    final_data.extend(data)\n","data = Dataset.from_list(final_data)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1739297130865,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"},"user_tz":-60},"id":"dQWGAoJxmbFL","outputId":"cff43c2b-4475-456a-b506-7cae0734b588"},"outputs":[{"output_type":"stream","name":"stdout","text":["BookRestaurant\n","PlayMusic\n","BookRestaurant\n","BookRestaurant\n","BookRestaurant\n","SearchScreeningEvent\n","RateBook\n","GetWeather\n","RateBook\n","SearchCreativeWork\n"]}],"source":["data = data.shuffle()\n","for i in range(10):\n","    print(data[i]['intent'])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"IsohD2JzjreB","executionInfo":{"status":"ok","timestamp":1739297132724,"user_tz":-60,"elapsed":1748,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["entity_set = set()\n","for idx in range(len(data)):\n","    for j in data[idx]['data']:\n","        slot_I = \"I-\" + str(j['entity'])\n","        entity_set.add(slot_I)\n","        slot_B = \"B-\" + str(j['entity'])\n","        entity_set.add(slot_B)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"PG9PjQ7akiCQ","executionInfo":{"status":"ok","timestamp":1739297134140,"user_tz":-60,"elapsed":1405,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["intent_set = set()\n","for idx in range(len(data)):\n","    intent_set.add(data[idx]['intent'])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"e_YuXDaAj-bF","executionInfo":{"status":"ok","timestamp":1739297134208,"user_tz":-60,"elapsed":24,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["entity_to_idx = {}\n","idx_to_entity = {}\n","for idx, ent in enumerate(entity_set):\n","    entity_to_idx[ent] = idx\n","    idx_to_entity[idx] = ent\n","entity_to_idx['O'] = 80\n","entity_to_idx[tokenizer.pad_token] = -100\n","idx_to_entity[-100] = tokenizer.pad_token\n","idx_to_entity[80] = 'O'"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1739297134232,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"},"user_tz":-60},"id":"df1TTo_KQO0u","outputId":"99d25038-6ec9-4cec-eea8-03732ba0cd0e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["82"]},"metadata":{},"execution_count":14}],"source":["len(entity_to_idx)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"KxH6tNu4mdj2","executionInfo":{"status":"ok","timestamp":1739297134264,"user_tz":-60,"elapsed":31,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["intent_to_idx = {}\n","idx_to_intent = {}\n","for idx, intent in enumerate(intent_set):\n","    intent_to_idx[intent] = idx\n","    idx_to_intent[idx] = intent"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"84ot57LP9rGM","executionInfo":{"status":"ok","timestamp":1739297137608,"user_tz":-60,"elapsed":17,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset as torchDataset\n","from torch.utils.data import DataLoader\n","\n","import torch\n","from torch.utils.data import Dataset as torchDataset\n","from torch.utils.data import DataLoader\n","\n","class SnipsDataset(torchDataset):\n","    \"\"\"\n","    Dataset for SNIPS NLU task.\n","    \"\"\"\n","    def __init__(self, data, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","\n","    def __getitem__(self, idx):\n","        sentence_tokens, slots_list = self.sample_example(idx)\n","\n","        inputs = self.tokenizer(\n","            sentence_tokens,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=50,\n","            return_tensors=\"pt\",\n","            is_split_into_words=True\n","        )\n","\n","        word_ids = inputs.word_ids()\n","        aligned_slots = []\n","        prev_word_id = None\n","\n","        for word_id in word_ids:\n","            if word_id is None:\n","                aligned_slots.append(-100)\n","            elif word_id != prev_word_id:\n","                aligned_slots.append(slots_list[word_id])\n","            else:\n","                base_slot = slots_list[word_id]\n","                if base_slot != entity_to_idx[\"O\"]:\n","                    entity_name = list(entity_to_idx.keys())[list(entity_to_idx.values()).index(base_slot)][2:]\n","                    aligned_slots.append(entity_to_idx[\"I-\" + entity_name])\n","                else:\n","                    aligned_slots.append(base_slot)\n","            prev_word_id = word_id\n","\n","        assert len(aligned_slots) == inputs[\"input_ids\"].shape[1], \"Slots and tokens length mismatch\"\n","\n","        return {\n","            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n","            \"slots\": torch.tensor(aligned_slots, dtype=torch.long),\n","            \"intent\": torch.tensor(intent_to_idx[self.data[idx][\"intent\"]], dtype=torch.long)\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def sample_example(self, idx):\n","        instance = self.data[idx]\n","        sentence_tokens = []\n","        slot_labels = []\n","\n","        for word_info in instance['data']:\n","            word_text = word_info['text']\n","            word_tokens = self.tokenizer.tokenize(word_text)\n","            sentence_tokens.extend(word_tokens)\n","\n","            if 'entity' in word_info and word_info['entity']:\n","                entity = word_info['entity']\n","                slot_list = [f\"I-{entity}\"] * len(word_tokens)\n","                slot_list[0] = f\"B-{entity}\"\n","            else:\n","                slot_list = [\"O\"] * len(word_tokens)\n","\n","            slot_labels.extend(slot_list)\n","\n","        sentence_tokens = [\"[CLS]\"] + sentence_tokens + [\"[SEP]\"]\n","        slot_labels = [\"O\"] + slot_labels + [\"O\"]\n","\n","        assert len(sentence_tokens) == len(slot_labels), \"Tokens and slots length mismatch!\"\n","\n","        return sentence_tokens, [entity_to_idx[slot] for slot in slot_labels]"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"wXpyeuc0El9K","executionInfo":{"status":"ok","timestamp":1739297445646,"user_tz":-60,"elapsed":22,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["import torch\n","\n","def custom_collate_fn(batch):\n","    input_ids = [item['input_ids'] for item in batch]\n","    attention_masks = [item['attention_mask'] for item in batch]\n","    slot_labels = [item['slots'] for item in batch]\n","    intent_labels = [item['intent'] for item in batch]\n","\n","    max_len = max(len(x) for x in input_ids)\n","\n","    def pad_sequence(seq, pad_value):\n","        return seq + [pad_value] * (max_len - len(seq))\n","\n","    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n","    input_ids = [pad_sequence(x.tolist(), pad_token_id) for x in input_ids]\n","\n","    attention_masks = [pad_sequence(x.tolist(), 0) for x in attention_masks]\n","\n","    slot_labels = [pad_sequence(x.tolist(), -100) for x in slot_labels]\n","\n","    input_ids = torch.tensor(input_ids, dtype=torch.long)\n","    attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n","    slot_labels = torch.tensor(slot_labels, dtype=torch.long)\n","    intent_labels = torch.tensor(intent_labels, dtype=torch.long)\n","\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_masks,\n","        'slots': slot_labels,\n","        'intent': intent_labels\n","    }"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"H0fQfKM8jz3F","executionInfo":{"status":"ok","timestamp":1739297447819,"user_tz":-60,"elapsed":15,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["import torch\n","from transformers import BertModel\n","\n","class JointBert(torch.nn.Module):\n","    def __init__(self, num_intent, num_slots,  bert_model=None):\n","        super(JointBert, self).__init__()\n","        if bert_model is None:\n","            self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n","        else:\n","            self.model = bert_model\n","        self.num_slots = num_slots\n","        self.num_intent = num_intent\n","\n","        self.dropout = torch.nn.Dropout(0.2)\n","        self.intent_classifier = torch.nn.Linear(768, num_intent)\n","        self.slot_classifier = torch.nn.Linear(768, num_slots)\n","\n","    def forward(self, input_ids, attention_mask, slot_labels=None, intent_labels=None):\n","        outputs = self.model(input_ids, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state  # (batch, seq_length, 768)\n","        hidden_states = self.dropout(hidden_states)\n","        # Intent Classification\n","        intent_logits = self.intent_classifier(hidden_states[:, 0, :])  # (batch, num_intents)\n","\n","        # Slots Classification\n","        slot_logits = self.slot_classifier(hidden_states)  # (batch, seq_length, num_slots)\n","\n","        if slot_labels is not None and intent_labels is not None:\n","            loss_fn_slots = torch.nn.CrossEntropyLoss(ignore_index=-100)\n","            loss_slot = loss_fn_slots(slot_logits.view(-1, self.num_slots), slot_labels.view(-1))\n","\n","            loss_fn_intent = torch.nn.CrossEntropyLoss()\n","            loss_intent = loss_fn_intent(intent_logits, intent_labels)\n","\n","            total_loss = loss_slot + loss_intent\n","\n","            return total_loss, intent_logits, slot_logits\n","        else:\n","            slot_preds = torch.argmax(slot_logits, dim=-1)\n","            intent_preds = torch.argmax(intent_logits, dim=-1)\n","            return intent_preds, slot_preds\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"jxiIg-10W6h6","executionInfo":{"status":"ok","timestamp":1739297455569,"user_tz":-60,"elapsed":25,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","import random\n","\n","random.seed(42)\n","random.shuffle(final_data)\n","\n","train_data, val_data = train_test_split(final_data, test_size=0.2, random_state=42)\n","\n","batch_size = 128\n","\n","train_dataloader = DataLoader(SnipsDataset(train_data, tokenizer), batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True)\n","val_dataloader = DataLoader(SnipsDataset(val_data, tokenizer), batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1739297449184,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"},"user_tz":-60},"id":"3hu4hiZqXs4l","outputId":"072e214d-724d-4443-b744-dbdd4117bfe1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 50])"]},"metadata":{},"execution_count":29}],"source":["next(iter(train_dataloader))['input_ids'].shape"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"6MG_tsziJATz","executionInfo":{"status":"ok","timestamp":1739297647184,"user_tz":-60,"elapsed":531,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["jb = JointBert(num_intent=len(intent_to_idx), num_slots=len(entity_to_idx))"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"D6NihQi2JXYW","executionInfo":{"status":"ok","timestamp":1739298973814,"user_tz":-60,"elapsed":468874,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}},"outputId":"7c4bfd5a-d2e6-454e-d5ff-028eb901629e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 finished | Avg Train Loss: 1.6140\n","ðŸ”¹ Validation | Loss: 0.2326 | Intent Acc: 0.9888 | Slot Acc: 0.9705\n","Epoch 1 finished | Avg Train Loss: 1.4758\n","ðŸ”¹ Validation | Loss: 0.2425 | Intent Acc: 0.9898 | Slot Acc: 0.9707\n","Epoch 2 finished | Avg Train Loss: 1.5539\n","ðŸ”¹ Validation | Loss: 0.2286 | Intent Acc: 0.9898 | Slot Acc: 0.9707\n","Epoch 3 finished | Avg Train Loss: 1.5637\n","ðŸ”¹ Validation | Loss: 0.2479 | Intent Acc: 0.9822 | Slot Acc: 0.9710\n","Epoch 4 finished | Avg Train Loss: 1.3523\n","ðŸ”¹ Validation | Loss: 0.2398 | Intent Acc: 0.9859 | Slot Acc: 0.9702\n","Epoch 5 finished | Avg Train Loss: 0.9289\n","ðŸ”¹ Validation | Loss: 0.2776 | Intent Acc: 0.9833 | Slot Acc: 0.9712\n","Epoch 6 finished | Avg Train Loss: 0.9119\n","ðŸ”¹ Validation | Loss: 0.2469 | Intent Acc: 0.9884 | Slot Acc: 0.9712\n","Epoch 7 finished | Avg Train Loss: 0.6840\n","ðŸ”¹ Validation | Loss: 0.2272 | Intent Acc: 0.9884 | Slot Acc: 0.9712\n","Epoch 8 finished | Avg Train Loss: 0.4094\n","ðŸ”¹ Validation | Loss: 0.2398 | Intent Acc: 0.9888 | Slot Acc: 0.9723\n","Epoch 9 finished | Avg Train Loss: 0.2843\n","ðŸ”¹ Validation | Loss: 0.2441 | Intent Acc: 0.9884 | Slot Acc: 0.9722\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","jb.to(device)\n","\n","epochs = 10\n","optimizer = torch.optim.Adam(lr=3e-5, params=jb.parameters())\n","\n","for epoch in range(epochs):\n","    epoch_loss = 0\n","    jb.train()\n","\n","    for idx, batch in enumerate(train_dataloader):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        slots = batch[\"slots\"].to(device)\n","        intent = batch[\"intent\"].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        total_loss_batch, intent_logits, slot_logits = jb(\n","            input_ids, attention_mask, slot_labels=slots, intent_labels=intent\n","        )\n","\n","        total_loss_batch.backward()\n","        optimizer.step()\n","\n","        epoch_loss += total_loss_batch.item()\n","\n","        # if idx % 20 == 0:\n","        #     print(f\"Epoch: {epoch} | Step: {idx} | Loss: {total_loss_batch.item()}\")\n","\n","    print(f\"Epoch {epoch} finished | Avg Train Loss: {epoch_loss:.4f}\")\n","\n","    jb.eval()\n","    val_loss = 0\n","    intent_correct, total_intent = 0, 0\n","    slot_correct, total_slot = 0, 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            slots = batch[\"slots\"].to(device)\n","            intent = batch[\"intent\"].to(device)\n","\n","            total_loss_batch, intent_logits, slot_logits = jb(\n","                input_ids, attention_mask, slot_labels=slots, intent_labels=intent\n","            )\n","\n","            val_loss += total_loss_batch.item() * input_ids.size(0)\n","            total_samples += input_ids.size(0)\n","\n","            intent_preds = intent_logits.argmax(dim=-1)\n","            intent_correct += (intent_preds == intent).sum().item()\n","            total_intent += intent.size(0)\n","\n","            valid_mask = slots != -100\n","            slot_preds = slot_logits.argmax(dim=-1)\n","            slot_correct += ((slot_preds == slots) & valid_mask).sum().item()\n","            total_slot += valid_mask.sum().item()\n","\n","    val_loss /= total_samples\n","    intent_acc = intent_correct / total_intent\n","    slot_acc = slot_correct / total_slot\n","\n","    print(f\"ðŸ”¹ Validation | Loss: {val_loss:.4f} | Intent Acc: {intent_acc:.4f} | Slot Acc: {slot_acc:.4f}\")"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"rOUg-v1hUUTo","executionInfo":{"status":"ok","timestamp":1739299148605,"user_tz":-60,"elapsed":32,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}}},"outputs":[],"source":["from sklearn.metrics import f1_score, accuracy_score\n","import torch\n","\n","def evaluate_model(jb, val_dataloader, device):\n","    jb.eval()\n","    intent_preds_all, intent_labels_all = [], []\n","    slot_preds_all, slot_labels_all = [], []\n","\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            slots = batch[\"slots\"].to(device)\n","            intent = batch[\"intent\"].to(device)\n","\n","            intent_preds, slot_preds = jb(input_ids, attention_mask)\n","\n","            # Intent Accuracy\n","            intent_preds = intent_preds.cpu().numpy()\n","            intent_labels = intent.cpu().numpy()\n","            intent_preds_all.extend(intent_preds)\n","            intent_labels_all.extend(intent_labels)\n","\n","            # Slot Predictions\n","            slot_preds = slot_preds.cpu().numpy()\n","            slot_labels = slots.cpu().numpy()\n","\n","            for pred, true in zip(slot_preds, slot_labels):\n","                valid_mask = true != -100\n","                slot_preds_all.append(pred[valid_mask].tolist())\n","                slot_labels_all.append(true[valid_mask].tolist())\n","\n","    # Intent Accuracy\n","    intent_acc = accuracy_score(intent_labels_all, intent_preds_all)\n","\n","    # Slot F1 (macro)\n","    slot_f1 = f1_score(\n","        [label for sublist in slot_labels_all for label in sublist],  # Flatten\n","        [pred for sublist in slot_preds_all for pred in sublist],  # Flatten\n","        average=\"macro\"\n","    )\n","\n","    correct_sentences = sum(\n","        (intent_preds_all[i] == intent_labels_all[i]) and\n","        (slot_preds_all[i] == slot_labels_all[i])\n","        for i in range(len(intent_preds_all))\n","    )\n","    sentence_acc = correct_sentences / len(intent_preds_all)\n","\n","    print(f\"ðŸ”¹ Validation Results:\")\n","    print(f\"Intent Accuracy: {intent_acc:.4f}\")\n","    print(f\"Slot F1 Score: {slot_f1:.4f}\")\n","    print(f\"Sentence-Level Semantic Frame Accuracy: {sentence_acc:.4f}\")\n","\n","    return intent_acc, slot_f1, sentence_acc"]},{"cell_type":"code","source":["intent_acc, slot_f1, sentence_acc = evaluate_model(jb, val_dataloader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q00bTTQhQMlA","executionInfo":{"status":"ok","timestamp":1739299155007,"user_tz":-60,"elapsed":4594,"user":{"displayName":"Anton Krasniuk","userId":"12003612234082770579"}},"outputId":"3f4dd956-a943-4761-b20a-df13c2dcaa2b"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”¹ Validation Results:\n","Intent Accuracy: 0.9884\n","Slot F1 Score: 0.9304\n","Sentence-Level Semantic Frame Accuracy: 0.8894\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOvRIIdpy5SG7aKSwAbToA9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}